Project:

I want to make data analysis app named 'Dallosh Analysis', to visualize the customers reclamation/complains for Telecom Company.
The goal is to automate the  processing of datasets, so we allow a data analyst on the app to upload his csv files, we will have the table preview, once cliked on add, it will add it to the quee, then the process will begin like it is doing a back processing, then it will show the result for for dataset like showing the the chart, pie, diagram, etc. for some KPI.
We work with dataset from twitter posts which has as columns: ['id', 'created_at', 'full_text', 'media', 'screen_name', 'name', 'profile_image_url', 'user_id', 'in_reply_to', 'retweeted_status', 'quoted_status', 'media_tags', 'favorite_count', 'retweet_count', 'bookmark_count', 'quote_count', 'reply_count', 'views_count', 'favorited', 'retweeted', 'bookmarked', 'url'], then once processed from our backend and microservices, we should have other columns such as 'sentiment', 'priority', 'main_topic'.

The interface should be modern and intuitive for data analyst for a company to take decisions to improve the quality for the customer service.

As pages we will have:
- the landing page as innovative startup for data analysts to automate their process with theirt datasets
- the auth(login/register) page
- profile page
- the dashboard for admin to manage users and roles , to upload csv files  to process on the backend side
- the home dashboard for data analyst role to view results, charts, etc , he could explore different datasets then kpi will display .

Note: The theme will be RED, we could also switch light/dark on the navbar.

We assume that:
- the RabbitMQ is already running on localhost:5672.
- the mongodb is running on localhost:27017

To create that app, here is the Folder Structure:

|-/
|-- .github/
|--- workflows/
|---- deployment.yaml  # if we want the all in one CI/CD
|---- backend_deployment.yaml  # if we want per project
|---- frontend_deployment.yaml  # if we want per project
|---- microservices_deployment.yaml # if we want per project
|-- docs/ # global docs
|-- backend/ # express js server with modular functions
|--- docs/
|--- src/
|---- api/
|----- auth/
|------ controler.ts  # login, register, refreshToken, me, deleteAccount, updateAccount
|------ service.ts
|------ intex.ts  # the the routes for the auth
|----- users/
|------ controler.ts  # crud method 
|------ service.ts
|------ intex.ts  # the the routes for the users, ( post, get, patch, delete)
|----- roles/
|------ controler.ts  # crud method
|------ service.ts
|------ intex.ts  # the the routes for the roles
|----- files/
|------ controler.ts  # upload, get file info, delete
|------ service.ts  
|------ intex.ts  # the the routes for the files
|----- tasks/    # 
|------ controler.ts # 
|------ service.ts # the service will work with  the database and also amqplib for RabbitMQ to send events on channel for queue task
|------ intex.ts  # the the routes for the tasks
|----- logs/   #
|------ controler.ts
|------ service.ts
|------ intex.ts  # the the routes for the logs
|----- settings/
|------ controler.ts
|------ service.ts
|------ intex.ts  # the the routes for the settings
|----- index.ts   # we import all the api routes
|---- common/ # reusable 
|----- services/   # example: BaseService
|----- middleware/  # example: the auth middleware
|------ auth.ts # auth midleware with simple jwt verification
|----- controllers/ # example: BaseController
|---- configs/
|----- contants.ts
|----- env.ts
|---- core/
|----- server/index.ts  # we init the express js 
|---- utils/  # some utils like generateUID, jwt sign or verify, etc.
|----- index.ts
|---- scripts/  # some scripts to run
|----- index.ts  
|----- collections.ts  # a script to create collections( 'users', 'files', 'tasks', 'logs', 'settings') if not exist yet in our database , for the collection setting if the doculment for the setting does not exit we create a document 
|----- root.ts # we could have a script to create the admin user if it does not exist in our mocked database.
|---- lib/  # our custom lib
|----- database/ # our custom database adapter for agnostic database providers
|------ base/   # BaseAdapter
|------ mongodb/ MongoDatabase
|------ index.ts # export DatabaseService
|----- index.ts
|---- types/
|----- schema/
|------ users.schema.ts
|------ files.schema.ts
|------ tasks.schema.ts
|------ logs.schema.ts
|------ settings.schema.ts
|---- main.ts  # we import the script, we import the core app server , we import all the api route from api we pass it on app, we import the database service from the lib and init it, then we call our script and we pass the database service,  then we pass the database service in the app.use(req, res, next) to share it for controllers which pass it for services, we create the bootstrap function to take configs
|---- index.ts  # we import the main.ts,  then we bootstrap the server on port 5006
|--- tests/
|---- units/ 
|---- e2e/
|--- .env
|--- .env.example
|--- .gitignore
|--- .dockerignore
|--- Dockerfile
|--- README.md
|--- package.json  # depenencies express js, jwt, multer for upload file, uuid for generating uid , amqplib for RabbitMQ server, mongodb for mongodb server  
|--- tsconfig.json
|-- frontend/ # we will make the frontend with next js
|--- docs/
|--- src/  # we follow the Next JS folder structure
|---- configs/
|----- contants.ts
|----- env.ts
|---- api/ # the server side route to fetch the files in the shareable  storage
|----- files.ts
|---- app/   #  app will be like our pages
|------ landing/
|------- page.tsx
|------ auth/
|------- layout.tsx  # tablayout 'login' and 'register'
|------- page.tsx
|------ profile/  # user profile
|------- page.tsx
|------ home/  # for the simple user 
|------- overview/
|-------- page.tsx
|------- datasets/  # files management
|-------- layout.tsx
|-------- page.tsx
|------- tasks/  # tasks management
|-------- layout.tsx # sidebar and right content
|-------- page.tsx
|------- analysis/  # analysic view
|-------- layout.tsx # sidebar and right content
|-------- page.tsx
|------ admin/  # for the admin
|------- overview/
|-------- page.tsx
|------- users/     # users management
|-------- layout.tsx  # tablayout 'users' and 'roles'
|-------- page.tsx
|------- datasets/  # files management
|-------- layout.tsx
|-------- page.tsx
|------- tasks/  # tasks management
|-------- layout.tsx # sidebar and right content
|-------- page.tsx
|------- analysis/  # analysic view
|-------- layout.tsx # sidebar and right content
|-------- page.tsx
|------- logs/  # activity logs for the platform
|-------- page.tsx
|------- settings/  #  app management
|-------- layout.tsx  # tablayout tab 'general' and tab 'ai configs'
|-------- page.tsx
|------- layout.py # sidebar, navbar, content
|------- page.tsx
|---- components/
|----- ui/ generic ui for all the pages
|----- layouts/
|------ AdminSidebar.tsx
|------ UserSidebar.tsx
|------ Navbar.tsx
|------ Header.tsx
|------ ...
|----- auth/
|------ Login.tsx # here is the list of permissions: 'manage_users', 'manage_datasets', 'manage_task', 'view_overview', 'read_users', 'read_datasets', 'read_tasks'. once logged in, we get the user and his role , if the role has permission 'manage_app' we lead the user to the admin dashboard, unless we lead the user to the home dashboard
|------ Register.tsx
|----- dashboard/
|------ users/
|------- UsersManaggment.tsx
|------- RolesManaggment.tsx
|------ datasets/
|------- DatasetsManagement.tsx
|------ tasks/
|------- SidebarTaskList.tsx
|------- TaskDetail.tsx   # task activity logs detail for the file and work with ampqlib, to listen event for the current file: 'added' -> 'in_queue' -> 'reading_dataset' -> 'process_cleaning' -> 'sending_to_llm' -> 'appending_collumns' -> 'saving_file' -> 'done', there will be also the 'on_error' event message, when on error there will be also another event for 'retry_step' to continue form the last step event, there will also have soem event to pause/resume/stop the task for the dataset
|------ logs/
|------- LogsActivities.tsx
|------ settings/
|------- GeneralSettings.tsx
|------- AISettings.tsx
|---- services/  # service for the consumer REST API with axios and the Next JS route api
|----- index.ts
|---- hooks/
|----- ...   
|---- stores/  # zustand
|----- index.ts
|----- ... 
|---- utils
|---- types/
|----- schema/
|------ users.schema.ts
|------ files.schema.ts
|------ tasks.schema.ts
|------ logs.schema.ts
|------ settings.schema.ts
|--- tests/
|---- units/
|---- e2e/
|--- .env.local
|--- .env.production
|--- .gitignore
|--- .dockerignore
|--- Dockerfile
|--- README.md
|--- package.json  # depenencies axios, amqplib for RabbitMQ server to listen event for tasks, zustand for store
|--- tsconfig.json
|-- microservices/  # running on background process
|--- auto_processing_datasets/  # we will have a queue task processing
|---- src/
|----- lib/
|------ database/  # we will use the database
|------- base/   # BaseAdapter
|------- mongodb/ # MongoDBAdapter
|------- __init__.py
|----- configs/
|----- utils/
|----- services/  # we create modular callback functions to be triggered for events , like reading file, cleaning, calling llm, appending columns, saving, retry_step 
|----- events/  # work with amqplib for RabbitMQ server  to listen events with the services callbacks and add it in task manager from task
|----- tasks/ #  task mananager for queue with uid:file_id, we could make resumeable task with celery library
|----- __init__.py
|---- venv/  # we init the venv, we could have libraries like pandas to read csv, pika work with RabbitMQ server to listen events, ollama for api, pymongo for mongodb server, celery for task management
|---- tests/
|----- units/
|----- e2e/
|---- .env
|---- .env.example
|---- .gitignore
|---- .dockerignore
|---- Dockerfile
|---- README.md
|---- main.py  # we import the from src and the lib database to pass for the service to work with it and  it will run on background
|-- storage/  # the local storage, the frontend, backend and microservices will have the shareable local storage folder,so we have to take into account that for each project (backend, frontend, microservices) they all fetch it outside their process working directory.
|--- datasets/  # the original files
|---- [file_id].csv
|--- cleaned/   # the cleaned files
|---- [file_id].csv
|--- analysed/  # the processed with llm files
|---- [file_id].csv
|-- docker-compose.yaml # for later deployment using docker compose
|-- README.md

Default: User credential:
 - admin:  admin@free.com | 123456
 - user:  admin@free.com | 123456 with role analyst
the database name: dallosh_analysis


The technical workflow for automation dataset processing:
- The user on the frontend upload the file, the backend we receive it and save it to the storage/datasets, it will generate the file info, then it will add on task manager database the file, then it will send back the response, when the user visit the task page on the frontend, he will see the list of files read to be started with staut 'added',  then he could click on start on the task detail, it will send the request to the backend which will then update the status of task for the file to 'in_queue', then will also send the event with data and ai config to the microservice to proceed, the microservice will then listen the event and the file path, then will proceed by reading and it will send event 'reading_dataset', cleaning , processing duplication, outlayers while it does send the event 'process_cleaning' then we save it to storage/cleaned/[file_id].csv, then the column which have the name 'full_text'  to send to the LLM via REST api with prompt via ollama to get the sentiment analysis: [negative, neutral, positive], priority[0, 1, 2] and topics that , so the response should be as json { analysis:[], priority:[], topics: [] } while it does send the event 'sending_to_llm', then on the completed dataframe we append the new column while we send event 'appending_columns', then we save it in storage/analysed/[file_id].csv while sending event 'saving_file' , then event 'done',   then it will send the request using database service adapter to update the database document for the concerned task for each step and while it does emit events for tasks status which will be usefull for the frontend user experience for progression.
- the microservice could also listen event from the pause/resume/stop for a task process, so the user could interrupt or resume it, but if stopped, it will run the next task instead and will stop the current task.

Note: 
- As we are sending  request to the external provider  for the LLM, we could face some rate limit or file limit or timeout,  so for the process , we will count the number of rows for the dataset, then we do pagination by 500 rows by default if not provide from the ai config to send sequentially and the number of the retyr in case there is an error will be 3 by default if not provide to the ai config.
- for the events we will have for topic 'tasks' for step event status:
  - 'added' -> 'in_queue' -> 'reading_dataset' -> 'process_cleaning' -> 'sending_to_llm' -> 'appending_collumns' -> 'saving_file' -> 'done' , there will be also the 'on_error' event message, when on error there will be also another event for 'retry_step' to continue form the last step event 
- the microservice will be intelligent when the backend sent the message on the event 'proceed_task', we will parse the message to json, then we will find the field msg.ai, there we get the msg.ai.preferences.mode if automatic, we find the field msg.ai.preferences.default_external_model_id, we get the model ai in the file msg.ai.external with filter where uid === default_external_model_id if the request does not work or the api  is wrong, we get the other models form external, if not still working then we go to default models. each model will also have the  retryRequests

 
As schema we will have:
- users: { uid, data:{email, password}, createdAt, createdBy, updatedAt, updatedBy}
- roles: { uid, data:{name, description, permissions: [ list of permissions]}, createdAt, createdBy, updatedAt, updatedBy},  here is the list of permissions: 'manage_users', 'manage_datasets', 'manage_tasks', 'manage_app', 'view_overview', 'read_users', 'read_datasets', 'read_tasks'.
- files: { uid, data:{ filename, size, file_path, extension, type}, createdAt, createdBy, updatedAt, updatedBy}
- tasks: { uid, data:{ file_id, file_path, status}, createdAt, createdBy, updatedAt, updatedBy}
- logs: {uid, data:{ method: 'post', 'get', 'patch', 'delete', path, response, requested_by: user_id | none}, createdAt, createdBy: 'system'}
- settings: {
   uid,
   data:{
	   general: {appName, appDescription, timeZone, isMaintance, etc},
	   ai: {
	     preferences:{
	       mode: 'local' | 'automatic' | 'external'
	       default_local_model_id: local.uid
	       default_external_model_id: external.uid
	     },
	     local:[
	        {uid, data:{model, baseUrl, apiKey, retryRequests: 3 | number max 10, paginateRowsLimit: 500 | number max 5000}, createdAt, createdBy, updatedAt, updatedBy},
	        
	     ],
	     external:[
	        { uid, data:{model, baseUrl, apiKey, retryRequests: 3 | number max 10, paginateRowsLimit: 500 | number max 5000}, createdAt, createdBy, updatedAt, updatedBy},
	     ],
	   }
   },
   createdAt, createdBy: 'system',
   updatedAt, updatedBy
}

So for our agnostic database we we will have:
# We assume, we have database: 
- collections: users:[], roles:[], permissions:[], files:[], tasks:[], activities:[], settings:[]
- The settings collection will have a document with fields: {uid, data:{ general:{appName, appDescription, }, ai_configs:{local:[], external:[], preferences:{mode, default_model_local_id, default_external_model_id}}}}, createdAt, createdBy: 'system', updatedAt, updatedBy }


For the events for the topic 'tasks':
- Event 'proceed_task',  the message sent  from to backend to the microservice: {file_id, file_path, ai:{preferences, local, external}}
- Event 'retry_step', the message from frontend to the microservice: {file_id, file_path, ai:{preferences, local, external}, last_event_step: 'in_queue' | 'reading_dataset' | 'process_cleaning' | 'sending_to_llm' | 'appending_collumns' | 'saving_file' | 'done' }, with it we could continue where we have to retry
- Event 'handle_process', the message sent from the frontend to the micorservice for resumeable task: {file_id, event: 'pause', 'resume' | 'stop'} , then on the microservice it will behave according to the event
- The rest of events step( 'added' | 'in_queue' | 'reading_dataset' | 'process_cleaning' | 'sending_to_llm' | 'appending_collumns' | 'saving_file' | 'done') , the message sent from the microservice to the topics for client side to join and listen: { file_id , event}.

Some relvant notes about the project:
- the RabbitMQ Python Docs: https://www.rabbitmq.com/tutorials/tutorial-one-python
- the RabbitMQ JS Docs: https://www.rabbitmq.com/tutorials/tutorial-one-javascript
- for the tasks and queue management with Celery python module: https://docs.celeryq.dev/en/stable/getting-started/introduction.html
- the UI for the Frontend we will use the ShadCN library: https://ui.shadcn.com/docs, for Next Js: https://ui.shadcn.com/docs/installation/next, there is also the tool MCP server for shadcn, you can call the available components in the shadcn registry
- ollama open ai compatible REST api client consumer python: https://pypi.org/project/ollama/

For the maintainability of the code, we must:
- write clean code
- write modularity for functions
- respect the SOLID and  DRY principle

