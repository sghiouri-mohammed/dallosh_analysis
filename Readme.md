# Dallosh Analysis

> An innovative data analysis platform for automating customer reclamation/complaint processing for Telecom Companies using AI-powered sentiment analysis and data visualization.

## í³‹ Table of Contents

- [Description](#description)
- [Features](#features)
- [Requirements](#requirements)
- [Technologies](#technologies)
- [Project Structure](#project-structure)
- [Getting Started](#getting-started)
- [Team](#team)
- [License](#license)

## í¾¯ Description

Dallosh Analysis is a comprehensive data analysis application designed to automate the processing of customer complaint datasets for Telecom Companies. The platform enables data analysts to upload CSV files containing Twitter posts, automatically processes them through AI-powered sentiment analysis, and provides intuitive visualizations including charts, pie diagrams, and KPIs.

The application processes Twitter datasets with columns such as `id`, `created_at`, `full_text`, `media`, `screen_name`, and various engagement metrics. After processing through the backend and microservices, additional columns are added: `sentiment`, `priority`, and `topic`.

### Key Workflow

1. **Upload**: Data analysts upload CSV files through the web interface
2. **Queue**: Files are added to a processing queue
3. **Process**: Automated background processing includes:
   - Data cleaning (removing emojis, special characters)
   - Sentiment analysis (negative, neutral, positive)
   - Priority classification (0, 1, 2)
   - Topic extraction
   - Column appending and file saving
4. **Visualize**: Results are displayed with interactive charts and KPIs

## âœ¨ Features

### Frontend
- **Modern UI**: Built with Next.js 16, React 19, and Tailwind CSS
- **Theme Support**: Red theme with light/dark mode switching
- **Role-Based Access Control**: Separate dashboards for admins and data analysts
- **Real-time Updates**: Live task progression tracking via RabbitMQ events
- **Data Visualization**: Interactive charts and diagrams using Recharts
- **Responsive Design**: Mobile-first approach with modern UX

### Backend
- **RESTful API**: Express.js server with modular architecture
- **JWT Authentication**: Secure token-based authentication
- **File Management**: CSV upload, preview, and download
- **Task Management**: Queue management for dataset processing
- **Activity Logging**: Comprehensive logging system
- **Settings Management**: Configurable AI models and storage options

### Microservices
- **Automated Processing**: Celery-based task processing
- **AI Integration**: Ollama LLM for sentiment analysis and topic extraction
- **Data Cleaning**: Intelligent text cleaning while preserving important data
- **Event-Driven**: RabbitMQ-based event communication
- **Resumable Tasks**: Pause, resume, and retry capabilities
- **Error Handling**: Robust error handling with retry mechanisms

## í³¦ Requirements

### Prerequisites
- **Node.js** 18+ (for backend and frontend)
- **Python** 3.10+ (for microservices)
- **MongoDB** 7.0+ (running on localhost:27017)
- **RabbitMQ** 3.x (running on localhost:5672)
- **Ollama** (for LLM processing)
- **Docker** & **Docker Compose** (optional, for containerized deployment)

### System Requirements
- Minimum 4GB RAM
- 10GB free disk space
- Internet connection (for downloading dependencies and models)

## í»  Technologies

### Frontend
- **Next.js 16** - React framework with App Router
- **React 19** - UI library
- **TypeScript** - Type safety
- **Tailwind CSS** - Utility-first CSS framework
- **Shadcn UI** - Accessible component library
- **Zustand** - State management
- **Axios** - HTTP client
- **Recharts** - Data visualization
- **AMQP Lib** - RabbitMQ client for real-time updates
- **PapaParse** - CSV parsing

### Backend
- **Express.js 5** - Web application framework
- **TypeScript** - Type safety
- **MongoDB** - Database
- **JWT** - Authentication
- **Multer** - File upload handling
- **AMQP Lib** - RabbitMQ integration
- **bcryptjs** - Password hashing
- **PapaParse** - CSV parsing

### Microservices
- **Python 3.10+** - Programming language
- **Celery** - Distributed task queue
- **RabbitMQ** - Message broker
- **Pandas** - Data manipulation
- **Ollama** - LLM API client
- **Pika** - RabbitMQ Python client
- **PyMongo** - MongoDB driver
- **Pytest** - Testing framework

### Infrastructure
- **Docker** - Containerization
- **Docker Compose** - Multi-container orchestration
- **Traefik** - Reverse proxy (production)
- **MongoDB** - Database
- **RabbitMQ** - Message broker
- **Ollama** - LLM server

## í³ Project Structure

\`\`\`
dalloh_analysis/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/          # CI/CD pipeline configurations
â”œâ”€â”€ backend/                # Express.js backend server
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api/           # API routes (auth, users, roles, files, tasks, logs, settings)
â”‚   â”‚   â”œâ”€â”€ common/        # Shared middleware and base classes
â”‚   â”‚   â”œâ”€â”€ configs/       # Configuration files
â”‚   â”‚   â”œâ”€â”€ core/          # Core server setup
â”‚   â”‚   â”œâ”€â”€ lib/           # Custom libraries (database adapter)
â”‚   â”‚   â”œâ”€â”€ scripts/       # Initialization scripts
â”‚   â”‚   â”œâ”€â”€ types/         # TypeScript type definitions
â”‚   â”‚   â””â”€â”€ utils/         # Utility functions
â”‚   â”œâ”€â”€ test/              # Tests
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ frontend/               # Next.js frontend application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/           # Next.js App Router pages
â”‚   â”‚   â”‚   â”œâ”€â”€ admin/     # Admin dashboard pages
â”‚   â”‚   â”‚   â”œâ”€â”€ home/      # User dashboard pages
â”‚   â”‚   â”‚   â”œâ”€â”€ auth/      # Authentication pages
â”‚   â”‚   â”‚   â””â”€â”€ landing/   # Landing page
â”‚   â”‚   â”œâ”€â”€ components/    # React components
â”‚   â”‚   â”œâ”€â”€ guards/        # Route protection guards
â”‚   â”‚   â”œâ”€â”€ services/      # API client services
â”‚   â”‚   â”œâ”€â”€ stores/        # Zustand state stores
â”‚   â”‚   â”œâ”€â”€ types/         # TypeScript type definitions
â”‚   â”‚   â””â”€â”€ configs/       # Configuration files
â”‚   â”œâ”€â”€ test/              # Tests
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ microservices/
â”‚   â””â”€â”€ auto_processing_datasets/  # Python microservice for dataset processing
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ celery_app.py     # Celery application
â”‚       â”‚   â”œâ”€â”€ configs/          # Configuration files
â”‚       â”‚   â”œâ”€â”€ events/           # RabbitMQ event listener
â”‚       â”‚   â”œâ”€â”€ lib/              # Database adapters
â”‚       â”‚   â”œâ”€â”€ services/         # Processing services
â”‚       â”‚   â”œâ”€â”€ tasks/            # Celery task definitions
â”‚       â”‚   â””â”€â”€ utils/            # Utility functions
â”‚       â”œâ”€â”€ test/                 # Tests
â”‚       â”œâ”€â”€ main.py               # Entry point
â”‚       â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ storage/                # Shared storage directory
â”‚   â”œâ”€â”€ datasets/          # Original uploaded files
â”‚   â”œâ”€â”€ cleaned/           # Cleaned dataset files
â”‚   â””â”€â”€ analysed/          # Processed dataset files
â”œâ”€â”€ models/                 # Local model storage
â”‚   â””â”€â”€ ollama/            # Ollama models
â”œâ”€â”€ docs/                   # Project documentation
â”œâ”€â”€ docker-compose.yaml     # Local development setup
â”œâ”€â”€ docker-compose.production.yaml  # Production setup
â””â”€â”€ README.md
\`\`\`

## íº€ Getting Started

### Quick Start with Docker Compose (Recommended)

1. **Clone the repository:**
   \`\`\`bash
   git clone <repository-url>
   cd dalloh_analysis
   \`\`\`

2. **Set up environment variables:**
   - Copy `.env.example` to `.env` in each service directory:
     - `backend/.env.example` â†’ `backend/.env`
     - `frontend/.env.local.example` â†’ `frontend/.env.local`
     - `microservices/auto_processing_datasets/.env.example` â†’ `microservices/auto_processing_datasets/.env`

3. **Start all services:**
   \`\`\`bash
   docker-compose up -d
   \`\`\`

4. **Access the application:**
   - Frontend: http://localhost:3006
   - Backend API: http://localhost:5006
   - RabbitMQ Management: http://localhost:15672 (admin/admin123)
   - MongoDB: localhost:27019

### Manual Setup

#### 1. Backend Setup

\`\`\`bash
cd backend
npm install
cp .env.example .env
# Edit .env with your configuration
npm run dev
\`\`\`

#### 2. Frontend Setup

\`\`\`bash
cd frontend
npm install
cp .env.local.example .env.local
# Edit .env.local with your API URL
npm run dev
\`\`\`

#### 3. Microservice Setup

\`\`\`bash
cd microservices/auto_processing_datasets
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
# Edit .env with your configuration

# Terminal 1 - Start Celery Worker
celery -A src.celery_app:celery_app worker --loglevel=info --queues=celery_processing_queue --pool=solo --concurrency=1

# Terminal 2 - Start Event Listener
python main.py
\`\`\`

### Default Credentials

- **Admin User:**
  - Email: `admin@free.com`
  - Password: `admin123`

- **Analyst User:**
  - Email: `user@free.com`
  - Password: `user123`

**âš ï¸ Important:** Change default passwords in production!

### Database Setup

The application will automatically create the following collections on first run:
- `users` - User accounts
- `roles` - User roles and permissions
- `files` - Uploaded dataset files
- `tasks` - Processing tasks
- `logs` - Activity logs
- `settings` - Application settings

### Environment Variables

#### Backend (.env)
\`\`\`env
NODE_ENV=development
PORT=5006
DB_TYPE=mongodb
DB_HOST=localhost
DB_PORT=27017
DB_NAME=dallosh_analysis
JWT_SECRET=your-secret-key-change-in-production
JWT_EXPIRES_IN=7d
RABBITMQ_URL=amqp://localhost:5672
STORAGE_PATH=../../storage
RABBITMQ_TOPIC_TASKS=tasks
\`\`\`

#### Frontend (.env.local)
\`\`\`env
NEXT_PUBLIC_API_URL=http://localhost:5006
NODE_ENV=development
\`\`\`

#### Microservice (.env)
\`\`\`env
DB_TYPE=mongodb
DB_HOST=localhost
DB_PORT=27017
DB_NAME=dallosh_analysis
STORAGE_DATASETS=./storage/datasets
STORAGE_CLEANED=./storage/cleaned
STORAGE_ANALYSED=./storage/analysed
RABBITMQ_HOST=localhost
RABBITMQ_PORT=5672
RABBITMQ_USER=admin
RABBITMQ_PASSWORD=admin123
CELERY_BROKER_URL=amqp://admin:admin123@localhost:5672//
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:1b
\`\`\`

## í±¥ Team

- **Ivan Joel SOBGUI**
- **Cyrile**
- **Pascal**
- **Ben Lol**
- **Mohammed**

## í³„ License

This project is licensed under the MIT License.

---

## í³š Additional Resources

- [Backend README](./backend/README.md)
- [Frontend README](./frontend/README.md)
- [Microservice README](./microservices/auto_processing_datasets/README.md)

## í´ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## í³ Support

For issues and questions, please open an issue on the GitHub repository.
